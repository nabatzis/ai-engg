{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70eee71a-b775-4db0-9d94-65eefa21e1fb",
   "metadata": {},
   "source": [
    "# RAG System using LLaMa Model\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10 or lower\n",
    "- macOS environment\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Create and Activate Virtual Environment\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python3.10 -m venv venv\n",
    "\n",
    "# Activate virtual environment\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### 2. Install PyTorch\n",
    "Install PyTorch, torchvision, and torchaudio first:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "\n",
    "### 3. Install Jupyter and Setup Kernel\n",
    "```bash\n",
    "# Install Jupyter and IPython kernel\n",
    "pip install jupyter ipykernel\n",
    "\n",
    "# Register the virtual environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=venv --display-name \"Python (venv)\"\n",
    "\n",
    "# Start Jupyter Lab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### 4. Additional Setup Notes\n",
    "- Make sure to select the \"Python (venv)\" kernel in your Jupyter notebook\n",
    "- The kernel name will appear in the top right corner of your notebook\n",
    "- You can switch kernels at any time using the kernel menu\n",
    "\n",
    "### Troubleshooting\n",
    "- If you encounter any issues with the kernel, try:\n",
    "  1. Restarting the kernel\n",
    "  2. Rerunning the kernel installation command\n",
    "  3. Verifying that your virtual environment is activated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499e51a-ccc9-4bac-9d0f-83fdd4938346",
   "metadata": {},
   "source": [
    "### Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497cf6a9-5341-4e84-931e-4d02bbe87724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: rank-bm25 in ./venv/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: llama-cpp-python in ./venv/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: fpdf in ./venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: ollama in ./venv/lib/python3.10/site-packages (0.4.7)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.48.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from rank-bm25) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in ./venv/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./venv/lib/python3.10/site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 sentence-transformers rank-bm25 llama-cpp-python fpdf ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ef4d55-0f0f-49ea-bc4e-e12d46fc340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in ./venv/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.10/site-packages (2.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic) (2.27.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "Found existing installation: ollama 0.4.7\n",
      "Uninstalling ollama-0.4.7:\n",
      "  Successfully uninstalled ollama-0.4.7\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./venv/lib/python3.10/site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in ./venv/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.4.7\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions pydantic\n",
    "!pip uninstall ollama -y\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e684402-2599-4440-9294-8cef1b935bb9",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc741fd0-cd74-4a30-ac23-71a9b17b1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8971b-4658-4529-be78-58c4da521f46",
   "metadata": {},
   "source": [
    "# PDFChunk Class\n",
    "`PDFChunk` is a data class that represents a segment of a PDF document. It contains:\n",
    "- `content`: The actual text content from the PDF\n",
    "- `page_num`: Page number where the content was found\n",
    "- `chunk_num`: Sequential ID for the chunk\n",
    "- `metadata`: Additional information like filename, title, author\n",
    "- `score`: Relevance score used during search (defaults to 0.0)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "chunk = PDFChunk(content=\"Sample text\", page_num=0, chunk_num=3, \n",
    "                 metadata={\"filename\": \"doc.pdf\"}, score=0.85)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5137fb61-aa0d-49a9-8a05-888669564e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDFChunk:\n",
    "    content: str\n",
    "    page_num: int\n",
    "    chunk_num: int\n",
    "    metadata: Dict[str, Any]\n",
    "    score: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a9ea6-155f-41da-acaf-8b7d9b3191a8",
   "metadata": {},
   "source": [
    "# PDFProcessor Class\n",
    "A utility class that processes PDF documents by breaking them into smaller, manageable chunks with configurable size (default 500 words) and overlap (default 50 words). It reads PDFs using PyPDF2, extracts metadata (filename, pages, title, author), and processes each page into overlapping text chunks stored as PDFChunk objects.\n",
    "\n",
    "```python\n",
    "processor = PDFProcessor(chunk_size=500, chunk_overlap=50)\n",
    "chunks = processor.process_pdf(\"document.pdf\")  # Returns list of PDFChunk objects\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b2ce4a-873e-490f-8fa4-3ba50acdfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    \"\"\"Handles PDF document processing and chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> List[PDFChunk]:\n",
    "        chunks = []\n",
    "        \n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = {\n",
    "                'filename': os.path.basename(pdf_path),\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'title': pdf_reader.metadata.get('/Title', ''),\n",
    "                'author': pdf_reader.metadata.get('/Author', '')\n",
    "            }\n",
    "            \n",
    "            # Process each page\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                page_chunks = self._chunk_text(text, page_num, metadata)\n",
    "                chunks.extend(page_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_text(self, text: str, page_num: int, metadata: Dict) -> List[PDFChunk]:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        chunk_num = 0\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_words = words[i:i + self.chunk_size]\n",
    "            if chunk_words:\n",
    "                chunk_text = ' '.join(chunk_words)\n",
    "                chunk = PDFChunk(\n",
    "                    content=chunk_text,\n",
    "                    page_num=page_num,\n",
    "                    chunk_num=chunk_num,\n",
    "                    metadata={**metadata, 'chunk_location': f'page_{page_num}_chunk_{chunk_num}'}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                chunk_num += 1\n",
    "        \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e6f21-db15-4737-b4a5-bcc5ed0e3a59",
   "metadata": {},
   "source": [
    "# EnhancedHybridRetriever Class\n",
    "A hybrid search system that combines semantic search (using SentenceTransformer embeddings, weighted at 70%) and lexical search (using BM25, weighted at 30%) to find relevant document chunks. It normalizes and combines both scores to provide the most relevant results, using `add_chunks()` to index content and `retrieve()` to get top-k matches.\n",
    "\n",
    "```python\n",
    "retriever = EnhancedHybridRetriever(semantic_weight=0.7)\n",
    "results = retriever.retrieve(\"query\", top_k=5)  # Returns top 5 relevant chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f967f30c-72b5-41c3-8e28-14caf61df028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHybridRetriever:\n",
    "    \"\"\"Enhanced retriever with PDF support and hybrid search\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 semantic_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                 semantic_weight: float = 0.7):\n",
    "        self.semantic_model = SentenceTransformer(semantic_model_name)\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.bm25 = None\n",
    "        self.chunks: List[PDFChunk] = []\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def add_chunks(self, chunks: List[PDFChunk]):\n",
    "        \"\"\"Index PDF chunks for hybrid search\"\"\"\n",
    "        self.chunks = chunks\n",
    "        texts = [chunk.content for chunk in chunks]\n",
    "        \n",
    "        # Create BM25 index\n",
    "        tokenized_texts = [text.lower().split() for text in texts]\n",
    "        self.bm25 = BM25Okapi(tokenized_texts)\n",
    "        \n",
    "        # Create dense embeddings\n",
    "        self.embeddings = self.semantic_model.encode(texts)\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[PDFChunk]:\n",
    "        \"\"\"Perform hybrid retrieval combining BM25 and semantic search\"\"\"\n",
    "        # BM25 scoring\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))\n",
    "        \n",
    "        # Normalize BM25 scores\n",
    "        bm25_scores = bm25_scores / bm25_scores.max() if bm25_scores.max() > 0 else bm25_scores\n",
    "        \n",
    "        # Semantic scoring\n",
    "        query_embedding = self.semantic_model.encode(query)\n",
    "        semantic_scores = np.dot(self.embeddings, query_embedding)\n",
    "        semantic_scores = semantic_scores / semantic_scores.max() if semantic_scores.max() > 0 else semantic_scores\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = (self.semantic_weight * semantic_scores + \n",
    "                         (1 - self.semantic_weight) * bm25_scores)\n",
    "        \n",
    "        # Get top-k candidates\n",
    "        top_k_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Update scores and return top chunks\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            chunk = self.chunks[idx]\n",
    "            chunk.score = float(combined_scores[idx])\n",
    "            results.append(chunk)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d87d8-d62e-4679-b436-d58f5bb0df65",
   "metadata": {},
   "source": [
    "# OllamaRAG Class\n",
    "A Retrieval-Augmented Generation (RAG) system that combines document retrieval with Ollama's LLM capabilities. It retrieves relevant document chunks using a hybrid retriever, constructs a prompt with the retrieved context, and generates an answer using Ollama, returning both the response and metadata about the used chunks.\n",
    "\n",
    "```python\n",
    "rag = OllamaRAG(model_name=\"llama2\", retriever=hybrid_retriever)\n",
    "result = rag.generate_response(\"How does X work?\", num_chunks=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607a1a4d-f371-43dc-838f-3bafb9f8f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaRAG:\n",
    "    \"\"\"RAG system using Ollama\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 retriever: EnhancedHybridRetriever,\n",
    "                 temperature: float = 0.7):\n",
    "        self.model_name = model_name\n",
    "        self.retriever = retriever\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def generate_response(self, query: str, num_chunks: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using retrieved context and Ollama\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retriever.retrieve(query, top_k=num_chunks)\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Page {chunk.page_num + 1}]: {chunk.content}\"\n",
    "            for chunk in relevant_chunks\n",
    "        ])\n",
    "        \n",
    "        # Construct prompt\n",
    "        prompt = f\"\"\"Use the following passages to answer the question. Include relevant page numbers in your response.\n",
    "        If you cannot find the answer in the passages, say so.\n",
    "\n",
    "        Passages:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # Generate response using Ollama\n",
    "        response = ollama.generate(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt,\n",
    "   #         temperature=self.temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'response': response['response'],\n",
    "            'used_chunks': [\n",
    "                {\n",
    "                    'content': chunk.content,\n",
    "                    'page': chunk.page_num + 1,\n",
    "                    'score': chunk.score,\n",
    "                    'filename': chunk.metadata['filename']\n",
    "                }\n",
    "                for chunk in relevant_chunks\n",
    "            ]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ba2f5-29c2-4933-ac7e-81ea60106845",
   "metadata": {},
   "source": [
    "# initialize_rag_system Function\n",
    "A setup function that creates a complete RAG system by initializing components (PDFProcessor and HybridRetriever), processing a set of expected PDF documents into chunks, indexing these chunks, and returning a configured OllamaRAG instance ready for question answering.\n",
    "\n",
    "```python\n",
    "rag_system = initialize_rag_system(docs_path=\"path/to/docs\", model_name=\"llama3.2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efc2e6e-d297-4d7c-8974-c137fde10bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_system(docs_path: str = \"/Users/Ritesh/Documents/dev/ai-engg/content/ai-engg/week2/docs\", model_name: str = \"llama3.2\") -> OllamaRAG:\n",
    "    \"\"\"Initialize the RAG system with documents from the specified path\"\"\"\n",
    "    # Initialize components\n",
    "    pdf_processor = PDFProcessor()\n",
    "    retriever = EnhancedHybridRetriever()\n",
    "    \n",
    "    # Expected PDF files\n",
    "    expected_pdfs = [\n",
    "        \"customer_documentation.pdf\",\n",
    "        \"incident_runbook.pdf\",\n",
    "        \"product_architecture.pdf\"\n",
    "    ]\n",
    "    \n",
    "    # Process PDFs\n",
    "    all_chunks = []\n",
    "    for pdf_name in expected_pdfs:\n",
    "        pdf_path = os.path.join(docs_path, pdf_name)\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find {pdf_name}. Please ensure it exists in the {docs_path} directory.\"\n",
    "            )\n",
    "        chunks = pdf_processor.process_pdf(pdf_path)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Index chunks\n",
    "    retriever.add_chunks(all_chunks)\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    return OllamaRAG(model_name, retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4cb27-3cb3-49ef-a130-f654d7fc1c39",
   "metadata": {},
   "source": [
    "# run_rag_demo Function\n",
    "A demonstration function that showcases the RAG system's capabilities by running predefined example queries and providing an interactive mode for custom questions. It initializes the system, processes queries, and displays results with source attribution and relevance scores, while also handling potential errors gracefully.\n",
    "\n",
    "```python\n",
    "run_rag_demo()  # Runs demo with example queries then enters interactive mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180de7cc-7165-4d96-b2bc-393d86e04bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_demo():\n",
    "    \"\"\"Demo the RAG system with example queries\"\"\"\n",
    "    try:\n",
    "        # Initialize the system\n",
    "        print(\"Initializing RAG system...\")\n",
    "        rag = initialize_rag_system()\n",
    "        \n",
    "        # Example queries\n",
    "        queries = [\n",
    "            \"What are the hardware requirements for TechnoVision's NeuroStack platform?\",\n",
    "            \"What happened during the March 10, 2024 outage at TechnoVision AI?\",\n",
    "            \"How do I set up a TechnoVision account and generate a TV-API-KEY?\"\n",
    "        ]\n",
    "        \n",
    "        # Run queries\n",
    "        for query in queries:\n",
    "            print(f\"\\nQuestion: {query}\")\n",
    "            result = rag.generate_response(query)\n",
    "            \n",
    "            print(\"\\nAnswer:\", result['response'])\n",
    "            print(\"\\nSources:\")\n",
    "            for chunk in result['used_chunks']:\n",
    "                print(f\"- {chunk['filename']}, Page {chunk['page']} (Score: {chunk['score']:.3f})\")\n",
    "            \n",
    "            input(\"\\nPress Enter for next question...\")\n",
    "        \n",
    "        # Interactive mode\n",
    "        print(\"\\nEntering interactive mode. Type 'exit' to quit.\")\n",
    "        while True:\n",
    "            query = input(\"\\nEnter your question: \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            result = rag.generate_response(query)\n",
    "            print(\"\\nAnswer:\", result['response'])\n",
    "            print(\"\\nSources:\")\n",
    "            for chunk in result['used_chunks']:\n",
    "                print(f\"- {chunk['filename']}, Page {chunk['page']} (Score: {chunk['score']:.3f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. Ollama is running locally (default port: 11434)\")\n",
    "        print(\"2. The required model is available (default: llama2)\")\n",
    "        print(\"3. PDF documents are present in the 'docs' directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f98f85-6d36-42bd-93ec-c0046b1c6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG system...\n",
      "\n",
      "Question: What are the hardware requirements for TechnoVision's NeuroStack platform?\n",
      "\n",
      "Answer: According to Page 1 of the passages, the hardware requirements for NeuroStack include:\n",
      "\n",
      "* TechnoVision Custom Silicon (TV-GPU-2024 series)\n",
      "* NeuroStack Accelerator Cards\n",
      "* Minimum 128GB of TechnoVision Certified Memory.\n",
      "\n",
      "Note that there are no other specific hardware requirements mentioned in the passage.\n",
      "\n",
      "Sources:\n",
      "- product_architecture.pdf, Page 1 (Score: 1.000)\n",
      "- incident_runbook.pdf, Page 1 (Score: 0.412)\n",
      "- customer_documentation.pdf, Page 1 (Score: 0.345)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter for next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What happened during the March 10, 2024 outage at TechnoVision AI?\n",
      "\n",
      "Answer: According to Passage [Page 1], on March 10, 2024, there was an outage due to a malfunction of the TechnoVision API rate limiter. The impact was significant, affecting 4 enterprise customers in the APAC region. To resolve this issue, an emergency patch (TV-Hotfix-2024-03) was applied.\n",
      "\n",
      "Sources:\n",
      "- incident_runbook.pdf, Page 1 (Score: 1.000)\n",
      "- customer_documentation.pdf, Page 1 (Score: 0.713)\n",
      "- product_architecture.pdf, Page 1 (Score: 0.712)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter for next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How do I set up a TechnoVision account and generate a TV-API-KEY?\n",
      "\n",
      "Answer: To set up a TechnoVision account and generate a TV-API-KEY, follow these steps:\n",
      "\n",
      "1. Register at console.technovision.ai (Page 1).\n",
      "2. Generate the TV-API-KEY from the Singapore portal.\n",
      "\n",
      "No specific page number is required for this answer as it can be inferred from the provided passages.\n",
      "\n",
      "Sources:\n",
      "- customer_documentation.pdf, Page 1 (Score: 1.000)\n",
      "- product_architecture.pdf, Page 1 (Score: 0.541)\n",
      "- incident_runbook.pdf, Page 1 (Score: 0.433)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_rag_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4ea08-6126-4a69-bb99-dca9d808f955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jt2h8K7uhZa"
      },
      "source": [
        "# RAG Implementation Workshop\n",
        "## Setting up RAG with Custom Documentation\n",
        "\n",
        "### Step 1: Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5GrH3qupuX"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers chromadb plotly scikit-learn anthropic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ3LQrqWuyO_"
      },
      "source": [
        "### Step 2: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYCaPNqBttB5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from typing import List, Dict\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "import anthropic\n",
        "from IPython.display import display, HTML, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsOzl0Udu5DA"
      },
      "source": [
        "### Step 3: Document Upload and Management"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look into GitHub repo and download those 3 ```.md``` documents"
      ],
      "metadata": {
        "id": "DdcoIqzQsdNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1n75GMHu6BA"
      },
      "outputs": [],
      "source": [
        "def setup_docs_directory():\n",
        "    \"\"\"Create docs directory if it doesn't exist\"\"\"\n",
        "    docs_dir = Path('docs')\n",
        "    docs_dir.mkdir(exist_ok=True)\n",
        "    return docs_dir\n",
        "\n",
        "def upload_docs_to_colab():\n",
        "    \"\"\"Upload markdown files to Colab\"\"\"\n",
        "    print(\"Please upload your markdown files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    docs_dir = setup_docs_directory()\n",
        "\n",
        "    # Move uploaded files to docs directory\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.md'):\n",
        "            dest_path = docs_dir / filename\n",
        "            with open(dest_path, 'wb') as f:\n",
        "                f.write(uploaded[filename])\n",
        "            print(f\"✅ Saved {filename} to docs directory\")\n",
        "\n",
        "    return list(uploaded.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEoKflAuvAr7"
      },
      "source": [
        "### Step 4: RAG Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx-iNcHDuAFf"
      },
      "outputs": [],
      "source": [
        "class EnhancedRAG:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.chroma_client = chromadb.Client(Settings(\n",
        "            persist_directory=\"./chroma_db\"\n",
        "        ))\n",
        "        self.collection = None\n",
        "        self.chunks = []\n",
        "        self.embeddings = []\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks and visualize the process.\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        # Visualize chunking\n",
        "        print(f\"\\n{'='*20} Chunking Visualization {'='*20}\")\n",
        "        print(f\"Total chunks created: {len(chunks)}\")\n",
        "\n",
        "        for idx, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
        "            print(f\"\\nChunk {idx + 1} Preview:\")\n",
        "            print(textwrap.fill(chunk[:200] + \"...\" if len(chunk) > 200 else chunk, width=80))\n",
        "            if idx == 2 and len(chunks) > 3:\n",
        "                print(\"\\n... and\", len(chunks) - 3, \"more chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def visualize_embeddings(self, embeddings: np.ndarray, chunks: List[str]):\n",
        "        \"\"\"Create an interactive visualization of embeddings.\"\"\"\n",
        "        pca = PCA(n_components=3)\n",
        "        embeddings_3d = pca.fit_transform(embeddings)\n",
        "\n",
        "        fig = go.Figure(data=[go.Scatter3d(\n",
        "            x=embeddings_3d[:, 0],\n",
        "            y=embeddings_3d[:, 1],\n",
        "            z=embeddings_3d[:, 2],\n",
        "            mode='markers+text',\n",
        "            text=[f\"Chunk {i+1}\" for i in range(len(chunks))],\n",
        "            hovertext=[chunk[:100] + \"...\" for chunk in chunks],\n",
        "            marker=dict(\n",
        "                size=10,\n",
        "                color=list(range(len(chunks))),\n",
        "                colorscale='Viridis',\n",
        "                opacity=0.8\n",
        "            )\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"3D Visualization of Document Embeddings\",\n",
        "            scene=dict(\n",
        "                xaxis_title=\"PCA Component 1\",\n",
        "                yaxis_title=\"PCA Component 2\",\n",
        "                zaxis_title=\"PCA Component 3\"\n",
        "            ),\n",
        "            width=800,\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "    def setup_collection(self, documents: List[Dict[str, str]]):\n",
        "        \"\"\"Initialize ChromaDB collection with provided documents.\"\"\"\n",
        "        # First, create chunks from documents\n",
        "        all_chunks = []\n",
        "        metadata_list = []\n",
        "        ids = []\n",
        "\n",
        "        print(\"\\n📄 Processing Documents and Creating Chunks...\")\n",
        "\n",
        "        for idx, doc in enumerate(documents):\n",
        "            chunks = self.chunk_text(doc[\"text\"])\n",
        "            self.chunks.extend(chunks)\n",
        "\n",
        "            # Create metadata and IDs for each chunk\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                metadata_list.append({\n",
        "                    \"source\": doc[\"source\"],\n",
        "                    \"chunk_index\": chunk_idx,\n",
        "                    \"original_doc_id\": idx\n",
        "                })\n",
        "                ids.append(f\"doc_{idx}_chunk_{chunk_idx}\")\n",
        "                all_chunks.append(chunk)\n",
        "\n",
        "        # Generate and store embeddings\n",
        "        print(\"\\n🔢 Generating Embeddings...\")\n",
        "        self.embeddings = self.model.encode(all_chunks)\n",
        "\n",
        "        # Visualize embeddings\n",
        "        self.visualize_embeddings(self.embeddings, all_chunks)\n",
        "\n",
        "        # Create or get collection\n",
        "        self.collection = self.chroma_client.get_or_create_collection(\n",
        "            name=\"enhanced_documentation_examples\"\n",
        "        )\n",
        "\n",
        "        # Add documents to collection\n",
        "        self.collection.add(\n",
        "            documents=all_chunks,\n",
        "            metadatas=metadata_list,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Successfully processed {len(documents)} documents into {len(all_chunks)} chunks\")\n",
        "        return self.collection\n",
        "\n",
        "    def retrieve_documents(self, query: str, n_results: int = 3):\n",
        "        \"\"\"Retrieve relevant documents and visualize the retrieval process.\"\"\"\n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results\n",
        "        )\n",
        "\n",
        "        # Visualize relevance scores\n",
        "        distances = results['distances'][0]\n",
        "        documents = results['documents'][0]\n",
        "        metadatas = results['metadatas'][0]\n",
        "\n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(\n",
        "                x=[f\"Chunk {meta['chunk_index']} from {meta['source']}\"\n",
        "                   for meta in metadatas],\n",
        "                y=[1 - dist for dist in distances],  # Convert distance to similarity\n",
        "                text=[f\"{(1-dist)*100:.1f}%\" for dist in distances],\n",
        "                textposition='auto',\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Relevance Scores for Query: '{query}'\",\n",
        "            xaxis_title=\"Document Chunks\",\n",
        "            yaxis_title=\"Relevance Score\",\n",
        "            yaxis_range=[0, 1],\n",
        "            width=800,\n",
        "            height=400\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_response(self, query: str, api_key: str):\n",
        "        \"\"\"Generate a response using Claude with retrieved context.\"\"\"\n",
        "        # Get relevant documents\n",
        "        results = self.retrieve_documents(query)\n",
        "        context = \"\\n\".join(results['documents'][0])\n",
        "        ### Please enter the Claude API key\n",
        "        api_key = \"\"\n",
        "        # Initialize Claude client\n",
        "        client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "        # Create the prompt\n",
        "        system_prompt = \"\"\"You are an AI assistant helping with Python documentation questions.\n",
        "        Use the provided context to answer questions, and maintain the style and terminology used in the context.\"\"\"\n",
        "\n",
        "        user_message = f\"\"\"Context from documentation:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Please provide a detailed answer based on the context provided.\"\"\"\n",
        "\n",
        "        # Generate response\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-5-haiku-20241022\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.2,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": user_message}]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRwYx_m4vJNL"
      },
      "source": [
        "### Step 5: Document Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2_FZRNWvL6J"
      },
      "outputs": [],
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"Handles loading documents for the RAG system.\"\"\"\n",
        "\n",
        "    def __init__(self, docs_dir: str = \"docs\"):\n",
        "        \"\"\"Initialize with the directory containing documentation files.\"\"\"\n",
        "        self.docs_dir = Path(docs_dir)\n",
        "\n",
        "    def load_documents(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Load all markdown files from the docs directory.\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        print(f\"📚 Loading documents from {self.docs_dir}\")\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        self.docs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load all .md files\n",
        "        for file_path in self.docs_dir.glob(\"*.md\"):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                documents.append({\n",
        "                    \"source\": file_path.name,\n",
        "                    \"text\": content\n",
        "                })\n",
        "                print(f\"✅ Loaded: {file_path.name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading {file_path}: {str(e)}\")\n",
        "\n",
        "        print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
        "        return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYoCrBH_vgBw"
      },
      "source": [
        "### Step 6: Interactive Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1UHpuVFvgs_"
      },
      "outputs": [],
      "source": [
        "class RAGSession:\n",
        "    \"\"\"Class to maintain RAG session state\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.rag = EnhancedRAG()\n",
        "\n",
        "        # Load documents\n",
        "        loader = DocumentLoader()\n",
        "        self.documents = loader.load_documents()\n",
        "\n",
        "        # Setup collection\n",
        "        print(\"\\n🚀 Initializing RAG System...\")\n",
        "        self.rag.setup_collection(self.documents)\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        \"\"\"Process a single query and return response\"\"\"\n",
        "        try:\n",
        "            print(\"\\n🔍 Retrieving relevant documents...\")\n",
        "            # This will show the visualization\n",
        "            response = self.rag.generate_response(query, self.api_key)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing query: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def run_rag_demo(api_key: str):\n",
        "    \"\"\"Run an interactive demo of the RAG system.\"\"\"\n",
        "    # Initialize RAG\n",
        "    print(\"\\n🚀 Initializing RAG System...\")\n",
        "    rag = EnhancedRAG()\n",
        "\n",
        "    # Load documents once\n",
        "    loader = DocumentLoader()\n",
        "    documents = loader.load_documents()\n",
        "\n",
        "    # Setup collection once\n",
        "    collection = rag.setup_collection(documents)\n",
        "    print(\"\\n✅ RAG System initialized and ready!\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Get query\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            query = input(\"Enter your question (or 'exit' to quit): \").strip()\n",
        "\n",
        "            if query.lower() == 'exit':\n",
        "                print(\"Thank you for using the RAG demo!\")\n",
        "                break\n",
        "\n",
        "            if not query:\n",
        "                continue\n",
        "\n",
        "            # Process query\n",
        "            print(f\"\\n🔍 Processing query: {query}\")\n",
        "\n",
        "            # This will show visualizations and generate response\n",
        "            response = rag.generate_response(query, api_key)\n",
        "\n",
        "            # Print response\n",
        "            print(\"\\n🤖 Response:\")\n",
        "            print(response)\n",
        "\n",
        "            # Wait for user before next query\n",
        "            print(\"\\nPress Enter for next question...\")\n",
        "            input()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error: {str(e)}\")\n",
        "            print(\"Let's try another question...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bmokEhDvkrM"
      },
      "source": [
        "### Step 7: Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0QkHAf7vnLt"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Step 1: Upload documents\n",
        "    print(\"Step 1: Upload your markdown files\")\n",
        "    uploaded_files = upload_docs_to_colab()\n",
        "    print(f\"\\nUploaded files: {uploaded_files}\")\n",
        "\n",
        "    # Step 2: Get API key\n",
        "    api_key = input(\"\\nPlease enter your Anthropic API key: \")\n",
        "\n",
        "    # Step 3: Run the demo\n",
        "    print(\"\\nStarting RAG demo...\")\n",
        "    run_rag_demo(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYifDCi3vp5R"
      },
      "source": [
        "### Run the Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hRzo5yfvs9e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86911d95-63f0-4086-8c96-f0fa25b166bb",
   "metadata": {},
   "source": [
    "# PDF RAG System Evaluation Framework\n",
    "\n",
    "This framework provides comprehensive evaluation capabilities for PDF-based Retrieval-Augmented Generation (RAG) systems. It assesses various aspects of RAG performance including relevance, faithfulness, coverage, and answer quality.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10 or lower\n",
    "- macOS environment\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Create and Activate Virtual Environment\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python3.10 -m venv venv\n",
    "\n",
    "# Activate virtual environment\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### 2. Install PyTorch\n",
    "Install PyTorch, torchvision, and torchaudio first:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "\n",
    "### 3. Install Jupyter and Setup Kernel\n",
    "```bash\n",
    "# Install Jupyter and IPython kernel\n",
    "pip install jupyter ipykernel\n",
    "\n",
    "# Register the virtual environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=venv --display-name \"Python (venv)\"\n",
    "\n",
    "# Start Jupyter Lab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### 4. Additional Setup Notes\n",
    "- Make sure to select the \"Python (venv)\" kernel in your Jupyter notebook\n",
    "- The kernel name will appear in the top right corner of your notebook\n",
    "- You can switch kernels at any time using the kernel menu\n",
    "\n",
    "### Troubleshooting\n",
    "- If you encounter any issues with the kernel, try:\n",
    "  1. Restarting the kernel\n",
    "  2. Rerunning the kernel installation command\n",
    "  3. Verifying that your virtual environment is activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c35511-7e3a-4de9-9f37-d8c97b95df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: rank-bm25 in ./venv/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: llama-cpp-python in ./venv/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: fpdf in ./venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: ollama in ./venv/lib/python3.10/site-packages (0.4.7)\n",
      "Requirement already satisfied: dataclasses in ./venv/lib/python3.10/site-packages (0.6)\n",
      "Requirement already satisfied: typing in ./venv/lib/python3.10/site-packages (3.7.4.3)\n",
      "Requirement already satisfied: rouge_score in ./venv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.48.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from rank-bm25) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in ./venv/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./venv/lib/python3.10/site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in ./venv/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 sentence-transformers rank-bm25 llama-cpp-python fpdf ollama dataclasses typing rouge_score tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5071205c-cf6a-4380-a48a-f98d27c7b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5a237-f873-488d-81fd-3ef2f34c82db",
   "metadata": {},
   "source": [
    "### PDFRAGResult Class\n",
    "```python\n",
    "@dataclass\n",
    "class PDFRAGResult:\n",
    "    query: str\n",
    "    contexts: List[Dict[str, Any]]\n",
    "    answer: str\n",
    "    ground_truth: str = \"\"\n",
    "    context_scores: List[float] = None\n",
    "```\n",
    "This dataclass encapsulates the results of a RAG query, including:\n",
    "- The original query\n",
    "- Retrieved contexts with PDF metadata\n",
    "- Generated answer\n",
    "- Ground truth (if available)\n",
    "- Context relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e819541-9bb8-452d-b9be-ddb6e9f56c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDFRAGResult:\n",
    "    query: str\n",
    "    contexts: List[Dict[str, Any]]  # Now includes PDF metadata\n",
    "    answer: str\n",
    "    ground_truth: str = \"\"\n",
    "    context_scores: List[float] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db900ef-84c4-4b7b-b91c-22776b0fd493",
   "metadata": {},
   "source": [
    "# PDF RAG Evaluator\n",
    "\n",
    "The `PDFRAGEvaluator` class is designed to assess how well a RAG (Retrieval-Augmented Generation) system works with PDF documents. It evaluates four main aspects:\n",
    "\n",
    "1. **Answer Quality**: How good is the generated answer compared to the expected answer?\n",
    "   - Uses ROUGE scores and semantic similarity\n",
    "   - Checks if the answer length is appropriate\n",
    "\n",
    "2. **Relevance**: Are the retrieved PDF chunks related to the question?\n",
    "   - Measures how well retrieved content matches the query\n",
    "   - Uses semantic similarity to score relevance\n",
    "\n",
    "3. **Faithfulness**: Does the answer stick to the source material?\n",
    "   - Checks if the answer uses information from the sources\n",
    "   - Detects if the answer makes up information not in the sources\n",
    "   - Verifies if the answer properly cites PDF pages\n",
    "\n",
    "4. **Context Coverage**: How well does the system use different sources?\n",
    "   - Measures if retrieved content is too repetitive\n",
    "   - Checks if information comes from various PDF sources\n",
    "\n",
    "The evaluator uses sentence transformers for semantic understanding and provides scores that help improve RAG system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a909dcd7-d436-413b-a7d3-7707c520c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFRAGEvaluator:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "    def evaluate_answer_quality(self, answer: str, ground_truth: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the quality of the generated answer against ground truth\n",
    "        \"\"\"\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = self.rouge_scorer.score(ground_truth, answer)\n",
    "        \n",
    "        # Calculate semantic similarity using embeddings\n",
    "        answer_embedding = self.embedding_model.encode([answer])[0]\n",
    "        truth_embedding = self.embedding_model.encode([ground_truth])[0]\n",
    "        semantic_similarity = cosine_similarity([answer_embedding], [truth_embedding])[0][0]\n",
    "        \n",
    "        # Calculate length ratio (answer length / ground truth length)\n",
    "        length_ratio = len(answer.split()) / len(ground_truth.split())\n",
    "        \n",
    "        return {\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "            'semantic_similarity': float(semantic_similarity),\n",
    "            'length_ratio': length_ratio\n",
    "        }\n",
    "\n",
    "    def evaluate_relevance(self, query: str, contexts: List[Dict[str, Any]]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Evaluate semantic relevance of retrieved contexts to the query\n",
    "        Now handles PDF chunks with metadata\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        context_texts = [ctx['content'] for ctx in contexts]\n",
    "        context_embeddings = self.embedding_model.encode(context_texts)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity([query_embedding], context_embeddings)[0]\n",
    "        \n",
    "        return similarities.tolist()\n",
    "\n",
    "    def evaluate_faithfulness(self, answer: str, contexts: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate if the generated answer is faithful to the retrieved contexts\n",
    "        Adapted for PDF chunks\n",
    "        \"\"\"\n",
    "        # Combine contexts\n",
    "        combined_context = \" \".join([ctx['content'] for ctx in contexts])\n",
    "        \n",
    "        # Calculate ROUGE scores between answer and context\n",
    "        rouge_scores = self.rouge_scorer.score(combined_context, answer)\n",
    "        \n",
    "        # Calculate lexical overlap\n",
    "        context_words = set(combined_context.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        overlap = len(context_words.intersection(answer_words)) / len(answer_words) if answer_words else 0\n",
    "        \n",
    "        # Check for potential hallucination (words in answer not in context)\n",
    "        novel_words = len(answer_words - context_words) / len(answer_words) if answer_words else 1\n",
    "        \n",
    "        # Check for source attribution\n",
    "        source_mentions = self._evaluate_source_attribution(answer, contexts)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "            'lexical_overlap': overlap,\n",
    "            'novelty_ratio': novel_words,\n",
    "            'source_attribution': source_mentions\n",
    "        }\n",
    "    \n",
    "    def _evaluate_source_attribution(self, answer: str, contexts: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if the answer properly attributes information to PDF sources\n",
    "        \"\"\"\n",
    "        # Extract page numbers and filenames mentioned in the answer\n",
    "        page_pattern = r\"page\\s+\\d+\"\n",
    "        mentioned_pages = set(re.findall(page_pattern, answer.lower()))\n",
    "        \n",
    "        # Get actual pages from contexts\n",
    "        actual_pages = set([f\"page {ctx['page']}\" for ctx in contexts])\n",
    "        \n",
    "        # Calculate attribution score\n",
    "        if not actual_pages:\n",
    "            return 0.0\n",
    "        \n",
    "        return len(mentioned_pages.intersection(actual_pages)) / len(actual_pages)\n",
    "    \n",
    "    def evaluate_context_coverage(self, contexts: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the diversity and redundancy of retrieved contexts\n",
    "        Adapted for PDF chunks\n",
    "        \"\"\"\n",
    "        if not contexts:\n",
    "            return {'diversity': 0.0, 'redundancy': 0.0, 'source_diversity': 0.0}\n",
    "            \n",
    "        # Get embeddings for all contexts\n",
    "        context_texts = [ctx['content'] for ctx in contexts]\n",
    "        embeddings = self.embedding_model.encode(context_texts)\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        n = len(contexts)\n",
    "        if n < 2:\n",
    "            return {'diversity': 1.0, 'redundancy': 0.0, 'source_diversity': 1.0}\n",
    "            \n",
    "        diversity_scores = []\n",
    "        redundancy_scores = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                similarity = similarities[i][j]\n",
    "                diversity_scores.append(1 - similarity)\n",
    "                redundancy_scores.append(similarity)\n",
    "        \n",
    "        # Calculate source diversity (unique PDFs referenced)\n",
    "        unique_sources = len(set([ctx['filename'] for ctx in contexts]))\n",
    "        source_diversity = unique_sources / len(contexts)\n",
    "                \n",
    "        return {\n",
    "            'diversity': np.mean(diversity_scores),\n",
    "            'redundancy': np.mean(redundancy_scores),\n",
    "            'source_diversity': source_diversity\n",
    "        }\n",
    "    \n",
    "    def evaluate_result(self, result: PDFRAGResult) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of a single PDF RAG result\n",
    "        \"\"\"\n",
    "        # Evaluate context relevance\n",
    "        relevance_scores = self.evaluate_relevance(result.query, result.contexts)\n",
    "        \n",
    "        # Evaluate answer faithfulness\n",
    "        faithfulness_metrics = self.evaluate_faithfulness(result.answer, result.contexts)\n",
    "        \n",
    "        # Evaluate context coverage\n",
    "        coverage_metrics = self.evaluate_context_coverage(result.contexts)\n",
    "        \n",
    "        # Evaluate answer quality if ground truth is available\n",
    "        quality_metrics = {}\n",
    "        if result.ground_truth:\n",
    "            quality_metrics = self.evaluate_answer_quality(result.answer, result.ground_truth)\n",
    "        \n",
    "        return {\n",
    "            'relevance': {\n",
    "                'scores': relevance_scores,\n",
    "                'mean_relevance': np.mean(relevance_scores)\n",
    "            },\n",
    "            'faithfulness': faithfulness_metrics,\n",
    "            'coverage': coverage_metrics,\n",
    "            'quality': quality_metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a772304-3def-4958-ba4b-c407bfb56b82",
   "metadata": {},
   "source": [
    "# PDF RAG Optimizer\n",
    "\n",
    "The `PDFRAGOptimizer` class analyzes evaluation results and suggests improvements for your RAG system. It looks for common issues and provides specific recommendations in four key areas:\n",
    "\n",
    "1. **Relevance Issues** (if score < 0.7)\n",
    "   - Suggests adjusting PDF chunk sizes\n",
    "   - Recommends fine-tuning search weights\n",
    "   - Advises reviewing PDF parsing\n",
    "\n",
    "2. **Hallucination Problems** (if novelty ratio > 0.3)\n",
    "   - Suggests reviewing chunk sizes\n",
    "   - Recommends adjusting LLM temperature\n",
    "   - Advises strengthening source attribution\n",
    "\n",
    "3. **Redundancy Issues** (if redundancy > 0.3)\n",
    "   - Suggests adjusting chunk overlap\n",
    "   - Recommends implementing deduplication\n",
    "   - Advises reviewing segmentation\n",
    "\n",
    "4. **Source Diversity Problems** (if diversity < 0.5)\n",
    "   - Suggests favoring different PDF sources\n",
    "   - Recommends reviewing relevance scoring\n",
    "   - Advises implementing diversity penalties\n",
    "\n",
    "Each suggestion includes practical steps to improve the specific aspect of RAG system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530cf628-85b8-48fa-bc1f-4aaf8e1a020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFRAGOptimizer:\n",
    "    def __init__(self, evaluator: PDFRAGEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        \n",
    "    def suggest_improvements(self, evaluation_results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Analyze evaluation results and suggest improvements for PDF RAG\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        # Analyze relevance\n",
    "        mean_relevance = evaluation_results['relevance']['mean_relevance']\n",
    "        if mean_relevance < 0.7:\n",
    "            suggestions.append(\n",
    "                \"Consider improving retrieval relevance:\\n\"\n",
    "                \"- Adjust chunk size for PDF processing\\n\"\n",
    "                \"- Fine-tune hybrid search weights\\n\"\n",
    "                \"- Review PDF parsing quality\"\n",
    "            )\n",
    "            \n",
    "        # Analyze faithfulness\n",
    "        if evaluation_results['faithfulness']['novelty_ratio'] > 0.3:\n",
    "            suggestions.append(\n",
    "                \"High novelty ratio indicates potential hallucination:\\n\"\n",
    "                \"- Review PDF chunk size\\n\"\n",
    "                \"- Adjust LLM temperature\\n\"\n",
    "                \"- Strengthen source attribution in prompts\"\n",
    "            )\n",
    "            \n",
    "        # Analyze coverage\n",
    "        if evaluation_results['coverage']['redundancy'] > 0.3:\n",
    "            suggestions.append(\n",
    "                \"High context redundancy detected:\\n\"\n",
    "                \"- Adjust chunk overlap in PDF processing\\n\"\n",
    "                \"- Implement cross-document deduplication\\n\"\n",
    "                \"- Review PDF segmentation strategy\"\n",
    "            )\n",
    "            \n",
    "        # Analyze source diversity\n",
    "        if evaluation_results['coverage']['source_diversity'] < 0.5:\n",
    "            suggestions.append(\n",
    "                \"Low source diversity:\\n\"\n",
    "                \"- Adjust retrieval to favor different PDF sources\\n\"\n",
    "                \"- Review document relevance scoring\\n\"\n",
    "                \"- Consider document-level diversity penalties\"\n",
    "            )\n",
    "        \n",
    "        return suggestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eda5e-5668-4683-98ea-212980515f64",
   "metadata": {},
   "source": [
    "# PDF RAG System Evaluation Function\n",
    "\n",
    "The `evaluate_pdf_rag_system` function is the main testing pipeline that runs and evaluates your RAG system. Here's what it does:\n",
    "\n",
    "1. **Test Execution**\n",
    "   - Takes a RAG system and test cases as input\n",
    "   - Runs each test query through the RAG system\n",
    "   - Collects responses and used contexts\n",
    "\n",
    "2. **Metric Collection**\n",
    "   - Tracks four key metrics:\n",
    "     - Relevance of retrieved content\n",
    "     - Faithfulness to source material\n",
    "     - Coverage/diversity of sources\n",
    "     - Answer quality (when ground truth available)\n",
    "\n",
    "3. **Results**\n",
    "   - Returns two sets of results:\n",
    "     - Detailed results for each test case\n",
    "     - Aggregated metrics (mean scores) across all tests\n",
    "\n",
    "This function helps you understand how well your RAG system performs across multiple test cases and different evaluation aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfd3732-48bb-423d-a144-2385b3078702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pdf_rag_system(rag_system: Any, test_cases: List[Dict[str, str]], \n",
    "                          evaluator: PDFRAGEvaluator) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a PDF RAG system using a set of test cases\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    metrics = {\n",
    "        'relevance': [],\n",
    "        'faithfulness': [],\n",
    "        'coverage': [],\n",
    "        'quality': []\n",
    "    }\n",
    "    \n",
    "    for test in tqdm(test_cases, desc=\"Evaluating test cases\"):\n",
    "        # Generate RAG response\n",
    "        response = rag_system.generate_response(test['query'])\n",
    "        \n",
    "        # Create evaluation result\n",
    "        result = PDFRAGResult(\n",
    "            query=test['query'],\n",
    "            contexts=response['used_chunks'],\n",
    "            answer=response['response'],\n",
    "            ground_truth=test.get('ground_truth', '')\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluation = evaluator.evaluate_result(result)\n",
    "        results.append((test, response, evaluation))\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics['relevance'].append(evaluation['relevance']['mean_relevance'])\n",
    "        metrics['faithfulness'].append(evaluation['faithfulness']['rougeL'])\n",
    "        metrics['coverage'].append(evaluation['coverage']['diversity'])\n",
    "        if evaluation['quality']:\n",
    "            metrics['quality'].append(evaluation['quality'].get('semantic_similarity', 0))\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    aggregate_metrics = {\n",
    "        'mean_relevance': np.mean(metrics['relevance']),\n",
    "        'mean_faithfulness': np.mean(metrics['faithfulness']),\n",
    "        'mean_coverage': np.mean(metrics['coverage']),\n",
    "        'mean_quality': np.mean(metrics['quality']) if metrics['quality'] else None\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'detailed_results': results,\n",
    "        'aggregate_metrics': aggregate_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8793d-7d80-46a9-a9ef-8dc562b9f0db",
   "metadata": {},
   "source": [
    "The `TECHNOVISION_TEST_CASES` is a collection of sample test queries designed to evaluate different aspects of the RAG system's performance with technical documentation. Each test case includes a query and its corresponding ground truth answer, covering various scenarios like hardware specifications, incident reports, API documentation, and regional system status. These test cases help verify if the RAG system can accurately retrieve and present information about technical requirements, system outages, API limitations, and region-specific issues from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab3c85b9-db6c-4613-86fe-c25ba42638dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test cases for TechnoVision AI documentation\n",
    "TECHNOVISION_TEST_CASES = [\n",
    "    {\n",
    "        'query': \"What are the hardware requirements for NeuroStack platform?\",\n",
    "        'ground_truth': \"TechnoVision Custom Silicon including TV-GPU-2024 series, NeuroStack Accelerator Cards, and minimum 128GB TechnoVision Certified Memory.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"What happened during the March 10, 2024 outage?\",\n",
    "        'ground_truth': \"TechnoVision API rate limiter malfunction caused an outage affecting 4 enterprise customers in APAC, resolved with Emergency patch TV-Hotfix-2024-03.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"What are the rate limits for TechnoVision's REST API?\",\n",
    "        'ground_truth': \"1000 requests per second per TV-API-KEY\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"What issues might a Singapore-based customer face in March 2024?\",\n",
    "        'ground_truth': \"Singapore cluster at 92% capacity, known bug in version 3.2.1-beta affecting Asian region deployments, and scheduled maintenance on March 25, 2024.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e8572-fd35-4943-b042-f1f9b17b3ffa",
   "metadata": {},
   "source": [
    "The `MockPDFRAGSystem` is a simplified implementation of a RAG system used for testing purposes. It maintains a predefined collection of PDF chunks organized by topics (hardware, outage, api, and singapore), where each chunk contains the actual content, filename, and page number. This structure simulates a real PDF-based knowledge base with technical documentation about TechnoVision's systems and services.\n",
    "\n",
    "The system implements a basic retrieval and response generation mechanism through its `generate_response` method. When given a query, it uses simple keyword matching to find relevant chunks from its collection, and then generates a response by concatenating the content of matched chunks. While this is a simplified approach compared to production RAG systems, it provides a practical way to test the evaluation framework with realistic technical content and document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb2a9ed-5e66-4110-a7cb-88c25d8f7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock RAG system for testing\n",
    "class MockPDFRAGSystem:\n",
    "    def __init__(self):\n",
    "        self.pdf_chunks = {\n",
    "            \"hardware\": [{\n",
    "                \"content\": \"TechnoVision Custom Silicon requirements include TV-GPU-2024 series and NeuroStack Accelerator Cards. Minimum 128GB TechnoVision Certified Memory required for optimal performance.\",\n",
    "                \"filename\": \"hardware_specs.pdf\",\n",
    "                \"page\": 1\n",
    "            }],\n",
    "            \"outage\": [{\n",
    "                \"content\": \"March 10, 2024 Incident Report: TechnoVision API rate limiter malfunction caused service disruption. Impact: 4 enterprise customers in APAC region affected. Resolution: Emergency patch TV-Hotfix-2024-03 deployed.\",\n",
    "                \"filename\": \"incident_report.pdf\",\n",
    "                \"page\": 1\n",
    "            }],\n",
    "            \"api\": [{\n",
    "                \"content\": \"TechnoVision REST API Rate Limits: Maximum 1000 requests per second per TV-API-KEY. Enterprise customers may request limit increases.\",\n",
    "                \"filename\": \"api_docs.pdf\",\n",
    "                \"page\": 1\n",
    "            }],\n",
    "            \"singapore\": [{\n",
    "                \"content\": \"Singapore Region Status (March 2024): Cluster utilization at 92% capacity. Known issues: Version 3.2.1-beta bug affecting Asian deployments. Scheduled maintenance: March 25, 2024.\",\n",
    "                \"filename\": \"region_status.pdf\",\n",
    "                \"page\": 1\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, query: str) -> dict:\n",
    "        # Simple keyword-based retrieval for demo\n",
    "        used_chunks = []\n",
    "        for key, chunks in self.pdf_chunks.items():\n",
    "            if key.lower() in query.lower():\n",
    "                used_chunks.extend(chunks)\n",
    "        \n",
    "        # Simple response generation by concatenating chunks\n",
    "        response = \" \".join([chunk[\"content\"] for chunk in used_chunks])\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"used_chunks\": used_chunks\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5288f9-e51f-48cd-bc6f-ca7531b66cab",
   "metadata": {},
   "source": [
    "# RAG Evaluation Runner\n",
    "\n",
    "The `run_rag_evaluation` function provides a complete pipeline for testing and analyzing a RAG system's performance. It executes in four main steps:\n",
    "\n",
    "1. **Setup**\n",
    "   - Creates the evaluator, optimizer, and mock RAG system\n",
    "   - Prepares all components for testing\n",
    "\n",
    "2. **Evaluation**\n",
    "   - Runs the evaluation using test cases\n",
    "   - Collects detailed metrics and results\n",
    "\n",
    "3. **Results Reporting**\n",
    "   - Shows aggregate metrics across all tests\n",
    "   - Displays optimization suggestions for improvement\n",
    "   - Prints detailed analysis of the first test case\n",
    "\n",
    "4. **Output Format**\n",
    "   - Metrics: relevance, faithfulness, coverage scores\n",
    "   - Suggestions for system improvements\n",
    "   - Comparison between generated answers and ground truth\n",
    "\n",
    "The function serves as a one-stop solution for running tests, analyzing performance, and getting actionable feedback for improving your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962c051a-a5e2-444e-9a40-e39f23b7f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluation system\n",
    "def run_rag_evaluation():\n",
    "    # Initialize components\n",
    "    evaluator = PDFRAGEvaluator()\n",
    "    optimizer = PDFRAGOptimizer(evaluator)\n",
    "    rag_system = MockPDFRAGSystem()\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"Starting RAG system evaluation...\")\n",
    "    evaluation_results = evaluate_pdf_rag_system(rag_system, TECHNOVISION_TEST_CASES, evaluator)\n",
    "    \n",
    "    # Print aggregate metrics\n",
    "    print(\"\\nAggregate Metrics:\")\n",
    "    for metric, value in evaluation_results['aggregate_metrics'].items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "    \n",
    "    # Get optimization suggestions\n",
    "    print(\"\\nOptimization Suggestions:\")\n",
    "    sample_detailed_eval = evaluation_results['detailed_results'][0][2]  # Get first test case evaluation\n",
    "    suggestions = optimizer.suggest_improvements(sample_detailed_eval)\n",
    "    for suggestion in suggestions:\n",
    "        print(f\"\\n{suggestion}\")\n",
    "    \n",
    "    # Print detailed results for first test case\n",
    "    print(\"\\nDetailed Results for First Test Case:\")\n",
    "    first_test = evaluation_results['detailed_results'][0]\n",
    "    test_case, response, evaluation = first_test\n",
    "    \n",
    "    print(f\"\\nQuery: {test_case['query']}\")\n",
    "    print(f\"Generated Answer: {response['response']}\")\n",
    "    print(f\"Ground Truth: {test_case['ground_truth']}\")\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Relevance: {evaluation['relevance']['mean_relevance']:.3f}\")\n",
    "    print(f\"Faithfulness (RougeL): {evaluation['faithfulness']['rougeL']:.3f}\")\n",
    "    print(f\"Coverage Diversity: {evaluation['coverage']['diversity']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1242c6-cbe1-4d13-97a9-5da0a151b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAG system evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test cases: 100%|███████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate Metrics:\n",
      "mean_relevance: 0.595\n",
      "mean_faithfulness: 1.000\n",
      "mean_coverage: 1.000\n",
      "mean_quality: 0.879\n",
      "\n",
      "Optimization Suggestions:\n",
      "\n",
      "Consider improving retrieval relevance:\n",
      "- Adjust chunk size for PDF processing\n",
      "- Fine-tune hybrid search weights\n",
      "- Review PDF parsing quality\n",
      "\n",
      "Detailed Results for First Test Case:\n",
      "\n",
      "Query: What are the hardware requirements for NeuroStack platform?\n",
      "Generated Answer: TechnoVision Custom Silicon requirements include TV-GPU-2024 series and NeuroStack Accelerator Cards. Minimum 128GB TechnoVision Certified Memory required for optimal performance.\n",
      "Ground Truth: TechnoVision Custom Silicon including TV-GPU-2024 series, NeuroStack Accelerator Cards, and minimum 128GB TechnoVision Certified Memory.\n",
      "\n",
      "Evaluation Metrics:\n",
      "Relevance: 0.640\n",
      "Faithfulness (RougeL): 1.000\n",
      "Coverage Diversity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_rag_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e145d5-b6f3-4011-b9e1-0b3b3105192f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
